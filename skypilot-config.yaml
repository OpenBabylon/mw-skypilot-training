name: h200_gcp

# Fields below describe each replica.
resources:
  cloud: gcp
  region: europe-west4
  # zone: us-central1-b # looks like "b" is where h200 are located
  ports: 8080-8100
  accelerators: H200:8
  instance_type: a3-ultragpu-8g
  use_spot: True
  # image_id is important to see h200 devices with nvidia-smi
  image_id: projects/deeplearning-platform-release/global/images/pytorch-2-4-cu124-v20250325-ubuntu-2204-py310

num_nodes: 8

# workdir: ./trainer

setup: |
  # git-lfs needed for hugging face integrations
  sudo apt update && sudo apt install -y git-lfs
  echo "SETUP DONE"
run: |
  env | grep -i skypilot
  # Example of output:
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_RAY_PORT=6380
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_RAY_HEAD_IP=10.128.0.67
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_NODE_IPS=10.128.0.67
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_TASK_ID=sky-2025-07-01-03-05-47-783941_sky-7947-antp_1
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_NUM_NODES=2
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_NODE_RANK=1
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_NUM_GPUS_PER_NODE=8
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_CLUSTER_INFO={"cluster_name": "sky-7947-antp", "cloud": "GCP", "region": "us-central1", "zone": "us-central1-b"}
  # (worker1, rank=1, pid=33053, ip=10.128.0.25) SKYPILOT_INTERNAL_JOB_ID=1
  # (head, rank=0, pid=39327) SKYPILOT_NODE_IPS=10.128.0.67
  # (head, rank=0, pid=39327) SKYPILOT_TASK_ID=sky-2025-07-01-03-05-47-783941_sky-7947-antp_1
  # (head, rank=0, pid=39327) SKYPILOT_NUM_NODES=2
  # (head, rank=0, pid=39327) SKYPILOT_NODE_RANK=0
  # (head, rank=0, pid=39327) SKYPILOT_NUM_GPUS_PER_NODE=8
  # (head, rank=0, pid=39327) SKYPILOT_CLUSTER_INFO={"cluster_name": "sky-7947-antp", "cloud": "GCP", "region": "us-central1", "zone": "us-central1-b"}
  # (head, rank=0, pid=39327) SKYPILOT_INTERNAL_JOB_ID=1


  if [ "$SKYPILOT_NODE_RANK" -eq 0 ]; then
    echo "HEAD NODE"
    export HEAD_IP=127.0.0.1
  else
    echo "WORKER NODE"
    export HEAD_IP=$SKYPILOT_RAY_HEAD_IP
  fi

  echo "HEAD NODE IPv4 address: "
  echo $HEAD_IP
  echo "========================"

  ## Setup python environment
  uv sync
  source .venv/bin/activate
  ## install flash-attention2
  uv pip install flash-attn --no-build-isolation
  export HF_HUB_ENABLE_HF_TRANSFER=1

  export NUM_PROCESSES=$(($SKYPILOT_NUM_NODES * $SKYPILOT_NUM_GPUS_PER_NODE))
  echo "num or processes:" $NUM_PROCESSES

  # Actual training launch
  accelerate launch --config_file trainer/cpt_config.yaml  \
      --num_machines=$SKYPILOT_NUM_NODES \
      --machine_rank=$SKYPILOT_NODE_RANK \
      --num_processes=$NUM_PROCESSES \
      --main_process_ip=$HEAD_IP \
      --main_process_port 14242 \
    trainer/cpt_train.py \
      --pretrained_model_name_or_path google/gemma-3-12b-pt \
      --train_dataset Goader/kobza \
      --replay_dataset=HuggingFaceFW/fineweb-edu \
      --mixing_probabilities=0.9,0.1 \
      --train_column=text \
      --throughput_measurement \
      --output_dir gemma3_cpt/test1/  \
      --repo_id polyagent/gemma3-cpt-test \
      --per_device_train_batch_size 2 \
      --gradient_accumulation_steps 4 \
      --max_seq_length 8192 \
      --precision_mode bfloat16 \
      --save_steps 3 \
      --logging_steps 10 \
      --max_steps=5 \
      --learning_rate=2e-4 \
      --warmup_ratio=0.06 \
      --weight_decay=0.1 \
      --lr_scheduler_type="cosine" \
      --optimizer="adamw_torch" \
      --attn_implementation=flash_attention_2 \
      --streaming
  echo "RUN DONE"
envs:
  HF_TOKEN: "<your HF token here"
  WANDB_API_KEY: "<your WandB token here"

